{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5016f49e",
   "metadata": {},
   "source": [
    "Note: This jupyter notebook was adapted from my initial work done on a Google Collab space, therefore some implementations reflect that.\n",
    "\n",
    "### Installations\n",
    "\n",
    "The following are the necessary installations required to run this notebook.\n",
    "\n",
    "Make sure you have the `requirements.txt` file!!!\n",
    "\n",
    "Change the path accordingly to where your file is stored\n",
    "\n",
    "#### Google Collab Steps\n",
    "\n",
    "**Mounting Google Drive**\n",
    "\n",
    "If on a Google Collab space, ensure to mount your Google drive to the Jupyter Notebook.\n",
    "\n",
    "```python\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "```\n",
    "\n",
    "**Install Requirements**\n",
    "\n",
    "```python\n",
    "%pip install -qU -r /content/requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b26b60c",
   "metadata": {},
   "source": [
    "#### Launching Xterm\n",
    "\n",
    "Xterm is a tool we will use to download open source Large Language Models that we can interact with.\n",
    "\n",
    "Please follow the following instructions **carefully**\n",
    "\n",
    "1. Run the code cell block\n",
    "\n",
    "2. After running the cell block you should be directed to the Xterm terminal\n",
    "\n",
    "3. Enter the following command in the console: `curl https://ollama.ai/install.sh | sh`\n",
    "\n",
    "    * This is a command that will install ollama, a platform that allows you to serve open source large language models\n",
    "\n",
    "    * This will take some time!\n",
    "\n",
    "4. After succesfully downloading ollama we will begin to run the ollama server using the command `ollama serve &`\n",
    "\n",
    "    * You should expect to see the following\n",
    "\n",
    "        * `Couldn't find '/root ...'`\n",
    "\n",
    "        * `Your new public key is:`\n",
    "\n",
    "5. Back in the terminal in the space available type in the command `ollama pull llama3`\n",
    "\n",
    "    * This should begin the process of downloading the open source large language model llama3\n",
    "\n",
    "    * This will take a while!\n",
    "\n",
    "6. Once the downloading is complete leave the terminal as is, you are now hosting the llama3 model through ollama and should be able to access it in the code steps\n",
    "\n",
    "Launching Xterm Cell Block\n",
    "\n",
    "```python\n",
    "%xterm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101a8655",
   "metadata": {},
   "source": [
    "#### Building Your Own Retrieval Augmented Generation Model\n",
    "\n",
    "In the following steps you will build a Retrieval Augmented Generation Model using the open source Large Language Model llama3.\n",
    "\n",
    "Please follow the steps carefully and answer all the provided questions IN YOUR OWN WORDS.\n",
    "\n",
    "There is no need to use AI to answer these questions, the focus is not to be correct but to solidify your understanding of the concepts behind your implementation.\n",
    "\n",
    "\n",
    "#### Setup\n",
    "\n",
    "Please follow this [documentation](https://console.groq.com/docs/quickstart) to setup your API key.\n",
    "\n",
    "Notes\n",
    "* You will need to create a Groq account\n",
    "* Once you have your Groq account you will need to create an API key that will allow you to access the LLMs stored on Groq\n",
    "* DO NOT SHARE THIS API KEY WITH ANYONE!!!\n",
    "* MAKE SURE YOU STORE THIS API KEY IN A SAFE PLACE!!!\n",
    "* BE CAREFUL WHEN PROJECTS THAT INCLUDE YOUR API KEY TO GITHUB!!!\n",
    "  * People search GitHub constantly for API KEYS that they can take advantage of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c1eb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "#this will ask you to input your api key if it is not already stored in the environment\n",
    "if not os.environ.get(\"GROQ_API_KEY\"):\n",
    "  os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter API key for Groq: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ad3f10",
   "metadata": {},
   "source": [
    "We will use Groq to access the llama3 model stored on their servers for some aspects of this project in order to speed up the process, as using the llama3 instance served on Xterm would take too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709a578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "#access the llama3 model stored on Groq\n",
    "llm = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205ae4d4",
   "metadata": {},
   "source": [
    "Answer the following questions in **your own words**\n",
    "\n",
    "##### Question 1: What are embeddings and what function do they serve?\n",
    "\n",
    "YOUR ANSWER HERE\n",
    "\n",
    "##### Question 2: Why do we need to access the llama3 embeddings specifically?\n",
    "\n",
    "YOUR ANSWER HERE\n",
    "\n",
    "Next we will access the OllamaEmbeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8220ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "#gather the ollama embeddings\n",
    "embeddings = OllamaEmbeddings(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceddb3a",
   "metadata": {},
   "source": [
    "##### Question 3: In your own words what do you *think* is happening in the line of code below\n",
    "\n",
    "YOUR ANSWER HERE\n",
    "\n",
    "##### Question 4: Run a quick google search on vector stores and what purpose they serve in Large Langauge Models and re-answer question 3 below.\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af782ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "#create a vector store\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a56be2",
   "metadata": {},
   "source": [
    "In this section we will load in the conversation file that you have edited\n",
    "* Make sure this is a txt file!\n",
    "* The path to the file that you upload might be different than the one stored below\n",
    "  * Find the path and update accordingly if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef761233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_unstructured import UnstructuredLoader\n",
    "\n",
    "#gather file paths for augmented data\n",
    "file_paths = [\n",
    "    \"/content/conversation.txt\"\n",
    "]\n",
    "#build a loader object to load those files\n",
    "loader = UnstructuredLoader(file_paths)\n",
    "#load in the files into documents to be accessed\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a2f0d2",
   "metadata": {},
   "source": [
    "Feeding entire files to train Large Language Models is extremely inefficient.\n",
    "\n",
    "Train LLMs uses the process of chunking, where we feed in \"chunks\" of data at a time instead of all at once. Although our file is relatively small, we will use best practices and chunk our information that we want to feed into the LLM.\n",
    "\n",
    "##### Question 5: Think of some the benefits of chunking and why we would need to do this to train Large Language Models\n",
    "\n",
    "ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2bd95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "#create a text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "\n",
    "#use the text splitter to split our document into chunks\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f81a0e5",
   "metadata": {},
   "source": [
    "Once our documents are split into chunks we store those chunks into the vector store that we created before.\n",
    "\n",
    "##### Question 6: Why would we need to have id's associated with each chunk of our training data?\n",
    "\n",
    "ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f4b835",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add documents into our vector store\n",
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "print(document_ids[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a407eb0d",
   "metadata": {},
   "source": [
    "The following is a visualization of the prompts that are being sent under the hood to the large language models.\n",
    "\n",
    "This is what the large language sees and uses to build a response.\n",
    "\n",
    "When you type in your question to Chat-GPT, something like the object below is sent and used to build the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d41ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "#pull in standard rag prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "#build an example prompt\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"(context goes here)\", \"question\": \"(question goes here)\"}\n",
    ").to_messages()\n",
    "\n",
    "assert len(example_messages) == 1\n",
    "#display the prompt\n",
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d364c6e1",
   "metadata": {},
   "source": [
    "The following class is used in the next methods to ensure that Data that is inputed to the RAG model is consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afead8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "#build state class that is used for retrieving and generating\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363afb4d",
   "metadata": {},
   "source": [
    "##### Question 7: In your own words describe what you think the function *similarity_search* is doing. What role do embeddings have in this function?\n",
    "\n",
    "ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc9f1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe4a0d6",
   "metadata": {},
   "source": [
    "##### Question 8: In your own words describe what the generate function is doing. What role does the output from the previous function *retrieve* have in this?\n",
    "\n",
    "ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e92629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a147c47",
   "metadata": {},
   "source": [
    "Below we build a LangChain Graph. You can think of this Graph as a way to create a workflow with multiple sequences. The image below shows you the steps within the created workflow. LangGraphs can be used with Agents but in this case we will use them with a RAG system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b68122",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchainVENV (3.8.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
