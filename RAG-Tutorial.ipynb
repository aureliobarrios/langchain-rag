{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5016f49e",
   "metadata": {},
   "source": [
    "Note: This jupyter notebook was adapted from my initial work done on a Google Collab space, therefore some implementations reflect that.\n",
    "\n",
    "### Installations\n",
    "\n",
    "The following are the necessary installations required to run this notebook.\n",
    "\n",
    "Make sure you have the `requirements.txt` file!!!\n",
    "\n",
    "Change the path accordingly to where your file is stored\n",
    "\n",
    "#### Google Collab Steps\n",
    "\n",
    "**Mounting Google Drive**\n",
    "\n",
    "If on a Google Collab space, ensure to mount your Google drive to the Jupyter Notebook.\n",
    "\n",
    "```python\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "```\n",
    "\n",
    "**Install Requirements**\n",
    "\n",
    "```python\n",
    "%pip install -qU -r /content/requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b26b60c",
   "metadata": {},
   "source": [
    "#### Launching Xterm\n",
    "\n",
    "Xterm is a tool we will use to download open source Large Language Models that we can interact with.\n",
    "\n",
    "Please follow the following instructions **carefully**\n",
    "\n",
    "1. Run the code cell block\n",
    "\n",
    "2. After running the cell block you should be directed to the Xterm terminal\n",
    "\n",
    "3. Enter the following command in the console: `curl https://ollama.ai/install.sh | sh`\n",
    "\n",
    "    * This is a command that will install ollama, a platform that allows you to serve open source large language models\n",
    "\n",
    "    * This will take some time!\n",
    "\n",
    "4. After succesfully downloading ollama we will begin to run the ollama server using the command `ollama serve &`\n",
    "\n",
    "    * You should expect to see the following\n",
    "\n",
    "        * `Couldn't find '/root ...'`\n",
    "\n",
    "        * `Your new public key is:`\n",
    "\n",
    "5. Back in the terminal in the space available type in the command `ollama pull llama3`\n",
    "\n",
    "    * This should begin the process of downloading the open source large language model llama3\n",
    "\n",
    "    * This will take a while!\n",
    "\n",
    "6. Once the downloading is complete leave the terminal as is, you are now hosting the llama3 model through ollama and should be able to access it in the code steps\n",
    "\n",
    "Launching Xterm Cell Block\n",
    "\n",
    "```python\n",
    "%xterm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101a8655",
   "metadata": {},
   "source": [
    "#### Building Your Own Retrieval Augmented Generation Model\n",
    "\n",
    "In the following steps you will build a Retrieval Augmented Generation Model using the open source Large Language Model llama3.\n",
    "\n",
    "Please follow the steps carefully and answer all the provided questions IN YOUR OWN WORDS.\n",
    "\n",
    "There is no need to use AI to answer these questions, the focus is not to be correct but to solidify your understanding of the concepts behind your implementation.\n",
    "\n",
    "\n",
    "#### Setup\n",
    "\n",
    "Please follow this [documentation](https://console.groq.com/docs/quickstart) to setup your API key.\n",
    "\n",
    "Notes\n",
    "* You will need to create a Groq account\n",
    "* Once you have your Groq account you will need to create an API key that will allow you to access the LLMs stored on Groq\n",
    "* DO NOT SHARE THIS API KEY WITH ANYONE!!!\n",
    "* MAKE SURE YOU STORE THIS API KEY IN A SAFE PLACE!!!\n",
    "* BE CAREFUL WHEN PROJECTS THAT INCLUDE YOUR API KEY TO GITHUB!!!\n",
    "  * People search GitHub constantly for API KEYS that they can take advantage of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c1eb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "#this will ask you to input your api key if it is not already stored in the environment\n",
    "if not os.environ.get(\"GROQ_API_KEY\"):\n",
    "  os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter API key for Groq: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ad3f10",
   "metadata": {},
   "source": [
    "We will use Groq to access the llama3 model stored on their servers for some aspects of this project in order to speed up the process, as using the llama3 instance served on Xterm would take too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709a578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "#access the llama3 model stored on Groq\n",
    "llm = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205ae4d4",
   "metadata": {},
   "source": [
    "Answer the following questions in **your own words**\n",
    "\n",
    "##### Question 1: What are embeddings and what function do they serve?\n",
    "\n",
    "YOUR ANSWER HERE\n",
    "\n",
    "##### Question 2: Why do we need to access the llama3 embeddings specifically?\n",
    "\n",
    "YOUR ANSWER HERE\n",
    "\n",
    "Next we will access the OllamaEmbeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8220ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "#gather the ollama embeddings\n",
    "embeddings = OllamaEmbeddings(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceddb3a",
   "metadata": {},
   "source": [
    "##### Question 3: In your own words what do you *think* is happening in the line of code below\n",
    "\n",
    "YOUR ANSWER HERE\n",
    "\n",
    "##### Question 4: Run a quick google search on vector stores and what purpose they serve in Large Langauge Models and re-answer question 3 below.\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89be465",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchainVENV (3.8.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
